{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b447c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fa2739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class NiniSiteForumCrawler:\n",
    "    \"\"\"\n",
    "    A web crawler for NiniSite forums.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, forum_number):\n",
    "        \"\"\"\n",
    "        Initializes the NiniSiteForumCrawler instance.\n",
    "\n",
    "        Args:\n",
    "            forum_number (int): The number of the forum to crawl.\n",
    "        \"\"\"\n",
    "        self.main_link = \"https://www.ninisite.com\"\n",
    "        self.forum_link = self.main_link + '/discussion/forum/' + str(forum_number)\n",
    "        self.initial_response = requests.get(self.forum_link)\n",
    "        \n",
    "    def get_number_of_pages_in_forum(self):\n",
    "        \"\"\"\n",
    "        Retrieves the number of pages in the forum.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of pages in the forum.\n",
    "        \"\"\"\n",
    "        response = self.initial_response\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            self.number_of_pages = int(soup.find_all('div', {'class': 'text-xs-center text-sm-left'})[0].find_all('li')[-2].text)\n",
    "        return self.number_of_pages\n",
    "    \n",
    "    def topic_header_parser(self, topic):\n",
    "        \"\"\"\n",
    "        Parses the topic header and returns a dictionary containing relevant information.\n",
    "\n",
    "        Args:\n",
    "            topic (bs4.element.Tag): The HTML tag representing a topic header.\n",
    "\n",
    "        Returns:\n",
    "            dict: Parsed topic information.\n",
    "        \"\"\"\n",
    "        parsed_topic = {\n",
    "            'subject': topic.find('span', {'class': 'topic_subject'}).text,\n",
    "            'link': self.main_link + topic.find('a')['href'],\n",
    "            'creator_username': topic.find('div', {'class': 'col-xs-12 p-x-0 pull-xs-right last-topic-user hidden-sm-down'}).text.split()[-1],\n",
    "            'creator_userid': topic.find('div', {'class': 'col-xs-12 p-x-0 pull-xs-right last-topic-user hidden-sm-down'}).find('a')['href'].split('/')[2],\n",
    "            'number_of_replies': int(topic.find('span', {'class': 'topic_number hidden-sm-up'}).text.split()[0])\n",
    "        }\n",
    "        return parsed_topic\n",
    "    \n",
    "    def crawl_topics_in_forum(self, starting_page=1, number_pages=10):\n",
    "        \"\"\"\n",
    "        Crawls topics in the forum and returns a DataFrame with the topic data.\n",
    "\n",
    "        Args:\n",
    "            starting_page (int): The starting page number to crawl (default: 1).\n",
    "            number_pages (int): The number of pages to crawl (default: 10).\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing the crawled topic data.\n",
    "        \"\"\"\n",
    "        self.topics_data = []\n",
    "        for pg_number in range(starting_page, number_pages):\n",
    "            print(pg_number)\n",
    "            html = requests.get(self.forum_link + f'?page={pg_number}').text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            topics = soup.find_all('div', {'class': 'col-xs-12 category--header p-x-0'})\n",
    "            for topic in topics:\n",
    "                self.topics_data.append(self.topic_header_parser(topic))\n",
    "                \n",
    "            time.sleep(1.5) \n",
    "        return pd.DataFrame(self.topics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14fd62cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "crawler = NiniSiteFroumCrawler(141)\n",
    "topics = crawler.crawl_topics_in_forum(starting_page=5,number_pages=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
